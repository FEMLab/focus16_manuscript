---
title: "f"
author: "Ben Nolan and  Nicholas Waters"
date: "10/7/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dada2); packageVersion("dada2")
library(ShortRead); packageVersion("ShortRead")
library(Biostrings); packageVersion("Biostrings")
SPEED = TRUE  # use  pregenerated files, dont plot if you can avoid it
if ( SPEED ){ load("./microbiome.RData")}
```
Here, we download the data from the Endobiota study.

```{bash}
while read x; do fasterq-dump --split-files $x ; done < ~/Downloads/SRR_Acc_List-1.txt

```

We follow a very generic DADA2 workflow:
```{r}
dirpath <- "./docs/microbiome_data/"
filtpath <- "./docs/microbiome_data/clean/"
mb_meta <- read.csv2("./docs/microbiome_data/metadata.tsv", stringsAsFactors = F,sep="\t",comment.char = '#')
sra_meta <- read.csv("./docs/microbiome_data/SraRunTable-PRJEB26800.txt", stringsAsFactors = F,sep=",")
meta <- full_join(
  mb_meta %>%select(sample_alias, sample_description),
  sra_meta %>% select(Run, Sample.Name), by=c("sample_alias"="Sample.Name")
)
meta$site <- gsub("(.*?) .*", "\\1", meta$sample_description)
meta$status <- gsub(".*\\((.*?)\\)$", "\\1", meta$sample_description)

# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
fnFs <- sort(list.files(dirpath, pattern="_1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(dirpath, pattern="_2.fastq.gz", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)


if (!SPEED) ggsave(plotQualityProfile(fnFs), filename = file.path(dirpath, "F_q.png"),dpi = 300, width = 16, height = 10, units = "in")
if (!SPEED)  ggsave(plotQualityProfile(fnRs), filename = file.path(dirpath, "R_q.png"),dpi = 300, width = 16, height = 10, units = "in")

baddiesF <-c(sample.names[grep("009",sample.names)])
baddiesR <-c(sample.names[grep("009",sample.names)])

sample.names <- sample.names[!sample.names %in% unique(c(baddiesF, baddiesR))]
for (baddie in c(baddiesF, baddiesR)){
  fnFs <- fnFs[!grepl(baddie, fnFs)]
  fnRs <- fnRs[!grepl(baddie, fnRs)]
}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(dirpath, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(dirpath, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

if (!SPEED) {
  out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs,trimLeft=30, trimRight=40,
                       maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                       compress=TRUE, multithread=TRUE)
} else {
  out <- list.files("./docs/microbiome_data/filtered")
}
head(out)


errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
if (!SPEED)  plotErrors(errF, nominalQ=TRUE)
derepF <- derepFastq(filtFs, verbose=TRUE)
derepR <- derepFastq(filtRs, verbose=TRUE)

dadaFs <- dada(derepF, err=errF, multithread=TRUE)
dadaRs <- dada(derepR, err=errR, multithread=TRUE)

merger <- mergePairs(dadaFs, derepF, dadaRs, derepR, verbose=TRUE)
seqtab <- makeSequenceTable(merger)
summary((nchar(getSequences(seqtab))))
hist(nchar(colnames(seqtab)))
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% 360:450]
hist(nchar(colnames(seqtab2)))
dim(seqtab2)

seqtab.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
sum(seqtab.nochim)/sum(seqtab2)
dim(seqtab.nochim)

getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(merger, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```
Now, we need to reformat the SILVA database for use with dada2. We could use the prebuild one, but as we have to build 2 (the normal SILVA 132 and our augmented one), we outline the steps here.

```{bash}
mkdir ~/Downloads/dada2_dbs/

zcat < ~/Downloads/SILVA_132_SSURef_tax_silva.fasta.gz | sed 's/>/> /g' | cut -f 1,3 -d " " | sed 's/ //g'  | awk '/^>/ {$0=$0 ";"}1' |gzip -c > ~/Downloads/dada2_dbs/silva.132.formated.fasta.gz
zcat < ~/Downloads/dada2_dbs/silva.132.formated.fasta.gz | awk 'BEGIN{RS=">";FS="\n"}NR>1{printf ">%s\n",$1;for (i=2;i<=NF;i++) {gsub(/U/,"T",$i); printf "%s\n",$i}}' |gzip -c > ~/Downloads/dada2_dbs/silva.132.dna.formated.fasta.gz

cp  ~/Downloads/SILVA_132_SSURef_tax_silva.fasta.gz  ~/Downloads/SILVA_132_SSURef_tax_silva_plus.fasta.gz
cat ./results/fast_focusDB_ribo16s.fasta |gzip -c  >>  ~/Downloads/SILVA_132_SSURef_tax_silva_plus.fasta.gz
zcat < ~/Downloads/SILVA_132_SSURef_tax_silva_plus.fasta.gz | sed 's/>/> /g' | cut -f 1,3 -d " " | sed 's/ //g'  | awk '/^>/ {$0=$0 ";"}1'  | gzip -c > ~/Downloads/dada2_dbs/silva.132.plus.formated.fasta.gz
zcat < ~/Downloads/dada2_dbs/silva.132.plus.formated.fasta.gz | awk 'BEGIN{RS=">";FS="\n"}NR>1{printf ">%s\n",$1;for (i=2;i<=NF;i++) {gsub(/U/,"T",$i); printf "%s\n",$i}}' |gzip -c > ~/Downloads/dada2_dbs/silva.132.plus.formated.dna.fasta.gz
```


Now, we can use DADA2's built-in command to create a species level dbs for both:
```{r, eval=FALSE}
dada2:::makeSpeciesFasta_Silva("~/Downloads/SILVA_132_SSURef_tax_silva.fasta.gz", "~/Downloads/dada2_dbs/silva_species_assignment_v132.fa.gz")
dada2:::makeSpeciesFasta_Silva("~/Downloads/SILVA_132_SSURef_tax_silva_plus.fasta.gz", "~/Downloads/dada2_dbs/silva_plus_species_assignment_v132.fa.gz")
#313502 sequences with genus/species binomial annotation output.
#319084 sequences with genus/species binomial annotation output.
```


That aside,we can assign taxanomic levels to each
```{r, cache=TRUE}

taxa_silva  <- assignTaxonomy(seqtab.nochim, "~/Downloads/dada2_dbs/silva.132.dna.formated.fasta.gz", multithread=TRUE)
taxa_silva_species <- assignSpecies(taxa_silva, "~/Downloads/silva_species_assignment_v132.fa.gz", allowMultiple = T)

taxa_silva_plus  <- assignTaxonomy(seqtab.nochim, "~/Downloads/dada2_dbs/silva.132.plus.formated.dna.fasta.gz", multithread=TRUE)
taxa_silva_plus_species <- assignSpecies(taxa_silva_plus, "~/Downloads/dada2_dbs/silva_plus_species_assignment_v132.fa.gz", allowMultiple = T)

```

The first time we ran the analysis (before having the focusDB results), here is what we did to ggenerate the species files, etc:
```{r, eval=FALSE}
taxa.print <- taxa_silva # Removing sequence rownames for display only

taxa.print[is.na(taxa.print[,1])] <- ""
taxa.print[is.na(taxa.print[,2])] <- ""
thesenames <- gsub("^ $", "", paste(taxa.print[,1], taxa.print[,2] ))
table(thesenames)

write.table(sort(unique(thesenames)), file = "./docs/microbiome_data/endo_species.txt", row.names = F,  col.names = F, quote=F)
fg <-  data.frame(taxa[,c(5,6)], stringsAsFactors = F)
rownames(fg) <- NULL
fg$cleangenus <- gsub("[-_](.*)", "", fg$Genus)
fg$cleanfamily <- gsub("[-_](.*)", "", fg$Family)
fg$cleangenus <- ifelse(grepl("\\d", fg$cleangenus),fg$cleanfamily, fg$cleangenus)
fg$cleangenus <- ifelse(is.na(fg$cleangenus),fg$cleanfamily, fg$cleangenus)
write.table(sort(unique(fg$cleangenus)), file = "./docs/microbiome_data/endo_genus.txt", row.names = F,  col.names = F, quote=F)
```


## Running FocusDB

Now that we know what genomes to be expecting, we ran focusDB on the genera found, and combined the results into a new database.


## Combining the DB:

```{r}
gg <- read.table("./docs/microbiome_data/endo_species.txt")
tt <-taxa_silva[1:10, ]

taxa_silva_both <- assignSpecies(taxa, "./docs/microbiome_data/combined.fasta", allowMultiple = T)

together <- merge(by="seq", all=TRUE,
                 cbind() data.frame(focus_genus=as.character(taxa_silva[,1]),
                             focus_species=as.character(taxa_silva[,2]),
                             seq = row.names(taxa_silva),
                             stringsAsFactors=FALSE),
                  data.frame(genus=as.character(taxa_silva_plus[,1]),
                             species=as.character(taxa_silva_plus[,2]),
                             seq = row.names(taxa_silva_plus),
                             stringsAsFactors=FALSE)
)
                  
taxa_silva_both_noname <- taxa_silva_both
rownames(taxa_silva_both_noname) <- NULL
taxa_silva_noname <- taxa_silva
rownames(taxa_silva_noname) <- NULL

taxa_silva_noname[is.na(taxa_silva_noname)] <- ""
taxa_silva_both_noname[is.na(taxa_silva_both_noname)] <- ""

table(taxa_silva_both_noname[,1] == taxa_silva_noname[, 1], useNA="always")
table(taxa_silva_both_noname[,2] == taxa_silva_noname[, 2], useNA="always")

```




# save for rerunning:

```{r}
save.image("./microbiome.RData")
```

Here, we can double check the read lengths of those we reejected for being to short
x <- p_summary_data %>% filter(grepl("short", messag)) %>% select(state)
cat(paste(unlist(x)))
```{bash eval=FALSE}
for i in SRR3924065 SRR3924066 SRR3924070 SRR3924069 SRR005679 SRR3924088 SRR3924090 SRR3926452 SRR4240105 SRR4240096 SRR4240104 SRR4240099 SRR4240101 SRR4240297 SRR4240295 SRR4240296 SRR2482990 SRR3926142 SRR5323657 SRR3926150 SRR3926138 SRR5323654 SRR3926531 SRR3926532 SRR005656 SRR3923709 DRR030207 DRR030209 DRR030208 DRR030206 DRR030211 DRR030210 SRR005676 SRR3926186 SRR3926185 SRR4239826 SRR3926134 SRR3926509 SRR3926450 SRR3926506 SRR4239827
SRR3926132 SRR3926152 SRR3926449 SRR3926133 SRR3926447 SRR3926153 SRR3926448 SRR3926508 SRR4239825 SRR4239974 SRR4239964 SRR4239962 SRR4239973 SRR4239968 SRR4239970 SRR4239969 SRR4239965 SRR4239967 SRR4239961 SRR4239963 SRR4239972 SRR4239971 SRR4239966 SRR3924296 SRR3924298 SRR3926755 SRR3926319 SRR3926324 SRR3926322 SRR3926323 SRR072399 SRR5320242 SRR3923869 SRR3923865 SRR3923868 SRR3923866 SRR5275895 SRR2102984 SRR060972 SRR3923850 SRR3923849 SRR3926776 SRR8448542 SRR3926775 SRR3923757 SRR3923756 SRR3925598 SRR3925597 SRR4240119 SRR4240118 SRR4240120 SRR3923884 SRR3923885 SRR4239979 SRR4239983 SRR4239982 SRR4239981 SRR4239980 SRR4239978 SRR4239985 SRR4239987 SRR4239986 SRR4239984 SRR3927509 SRR029223 SRR029225 SRR3927350; do awk '{if(NR%4==2) {count++; bases += length} } END{print bases/count}' ../.focusDB/$i/${i}_1.fastq >> shorties ; done
```


## Long length read data

With short read data, we do not identify more sequences, but we wantred to see how it would perfom on full-length 16s from the same physilogical location. We identified a  dattaset from wagner 2016: <https://bmcmicrobiol.biomedcentral.com/articles/10.1186/s12866-016-0891-4>.  We downloaded those reads from ENA, and converted to fastq with PacBio's <https://github.com/PacificBiosciences/pbh5tools>.

```{bash, eval=FALSE} 
conda create -n pacdada pbh5tools bax2bam lima ; conda activate pacdada

bax2bam -o microbiome ./m150128_105130_00127_c100679732550000001823135702221544_s1_p0.bas.h5


bash5tools.py  --outFilePref myreads --outType fastq  --minLength 1400 ./m150128_105130_00127_c100679732550000001823135702221544_s1_p0.bas.h5
conda deactivate 
conda create -n demul && conda activate demul && pip install demultiplex biopython==1.72

```

These were then taken through the dada2 pipeline, following practices laid out here: <https://benjjneb.github.io/LRASManuscript/LRASms_Zymo.html>, using primer seqeunces from the supplementary information:

```{r}
primers <-  read.csv2(text="PRIMER_NAME BARCODE PRIMER
BACT16s_v1_0001_Forward TCAGACGATGCGTCAT AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0002_Forward CTATACATGACTCTGC AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0003_Forward TACTAGAGTAGCACTC AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0004_Forward TGTGTATCAGTACATG AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0005_Forward ACACGCATGACACACT AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0006_Forward GATCTCTACTATATGC AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0007_Forward ACAGTCTATACTGCTG AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0008_Forward ATGATGTGCTACATCT AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0009_Forward CTGCGTGCTCTACGAC AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0010_Forward GCGCGATACGATGACT AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0011_Forward CGCGCTCAGCTGATCG AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0012_Forward GCGCACGCACTACAGA AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0013_Forward ACACTGACGTCGCGAC AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0014_Forward CGTCTATATACGTATA AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0015_Forward ATAGAGACTCAGAGCT AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0016_Forward TAGATGCGAGAGTAGA AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0017_Forward CATAGCGACTATCGTG AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0018_Forward CATCACTACGCTAGAT AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0019_Forward CGCATCTGTGCATGCA AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0020_Forward TATGTGATCGTCTCTC AGMGTTYGATYMTGGCTCAG
BACT16s_v9_0001_Reverse ATGACGCATCGTCTGA ACGGYTACCTTGTTACGACTT
BACT16s_v9_0002_Reverse GCAGAGTCATGTATAG ACGGYTACCTTGTTACGACTT
BACT16s_v9_0003_Reverse GAGTGCTACTCTAGTA ACGGYTACCTTGTTACGACTT
BACT16s_v9_0004_Reverse CATGTACTGATACACA ACGGYTACCTTGTTACGACTT
BACT16s_v9_0005_Reverse AGTGTGTCATGCGTGT ACGGYTACCTTGTTACGACTT
BACT16s_v9_0006_Reverse GCATATAGTAGAGATC ACGGYTACCTTGTTACGACTT
BACT16s_v9_0007_Reverse CAGCAGTATAGACTGT ACGGYTACCTTGTTACGACTT
BACT16s_v9_0008_Reverse AGATGTAGCACATCAT ACGGYTACCTTGTTACGACTT
BACT16s_v9_0009_Reverse GTCGTAGAGCACGCAG ACGGYTACCTTGTTACGACTT
BACT16s_v9_0010_Reverse AGTCATCGTATCGCGC ACGGYTACCTTGTTACGACTT
BACT16s_v9_0011_Reverse CGATCAGCTGAGCGCG ACGGYTACCTTGTTACGACTT
BACT16s_v9_0012_Reverse TCTGTAGTGCGTGCGC ACGGYTACCTTGTTACGACTT
BACT16s_v9_0013_Reverse GTCGCGACGTCAGTGT ACGGYTACCTTGTTACGACTT
BACT16s_v9_0014_Reverse TATACGTATATAGACG ACGGYTACCTTGTTACGACTT
BACT16s_v9_0015_Reverse AGCTCTGAGTCTCTAT ACGGYTACCTTGTTACGACTT
BACT16s_v9_0016_Reverse TCTACTCTCGCATCTA ACGGYTACCTTGTTACGACTT
BACT16s_v9_0017_Reverse CACGATAGTCGCTATG ACGGYTACCTTGTTACGACTT
BACT16s_v9_0018_Reverse ATCTAGCGTAGTGATG ACGGYTACCTTGTTACGACTT
BACT16s_v9_0019_Reverse TGCATGCACAGATGCG ACGGYTACCTTGTTACGACTT
BACT16s_v9_0020_Reverse GAGAGACGATCACATA ACGGYTACCTTGTTACGACTT
", sep=" ", stringsAsFactors=F)
write.table(primers, file = "microbiome_primers.tab", sep = "\t", quotes=F, row.names = F)
```

As the forward barcode is the same as the reverese, we reformed the primers into a fasta file required by lima <https://github.com/PacificBiosciences/barcoding>.  We then brought the strains through a bax2bam  -> lima  -> bam2fastq pipeline

```{bash eval=FALSE}
conda install  pbccs==3.4 bax2bam bam2fastx lima
for i in ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR144/ERR1447466/m150128_105130_00127_c100679732550000001823135702221544_s1_p0.2.bax.h5 ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR144/ERR1447466/m150128_105130_00127_c100679732550000001823135702221544_s1_p0.3.bax.h5 ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR144/ERR1447466/m150128_105130_00127_c100679732550000001823135702221544_s1_p0.bas.h5 ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR144/ERR1447466/m150128_105130_00127_c100679732550000001823135702221544_s1_p0.metadata.xml ; do wget $i ; done

bax2bam -o microbiome ./m150128_105130_00127_c100679732550000001823135702221544_s1_p0.bas.h5
ccs ./microbiome.subreads.bam microbiome.ccs.bam
lima --log-level TRACE --same -ccs  ./microbiome.ccs.bam ./primers.fasta ./demuxed_same_ccs.bam
bam2fastq --split-barcodes -o ERS1190962_ccs  ./demuxed_same_ccs.bam
mkdir ERS1190962_ccs/
mv ERS1190962_ccs*.fastq.gz ./ERS1190962_ccs/
tar czf ERS1190962_css.tar.gz ERS1190962_ccs/

# download to local computer from $SCRATCH/pacbio

```


Then, we follow the tutorial:

```{r}
dpath <- "~/Downloads/ERS1190962_ccs_limits/" # CHANGE ME to location of the fastq file
Fprimer <- "AGMGTTYGATYMTGGCTCAG"
Rprimer <- "ACGGYTACCTTGTTACGACTT"

#rc <- dada2:::rc
# theme_set(theme_bw())
# genusPalette <- c(Bacillus="#e41a1c", Enterococcus="#377eb8", Escherichia="#4daf4a", Lactobacillus="#984ea3",
#                   Listeria="#ff7f00", Pseudomonas="#ffff33", Salmonella="#a65628", Staphylococcus="#f781bf")
pacF <- sort(list.files(dpath, pattern=".fastq.gz", full.names = TRUE))
sample.names <- sapply(strsplit(basename(pacF), "_"), `[`, 3)

nop <- sort(file.path(dpath, "noprimers", basename(pacF)))
prim <- removePrimers(pacF, nop, primer.fwd=Fprimer, primer.rev=dada2:::rc(Rprimer), orient=TRUE, verbose=TRUE)
hist(nchar(dada2::getSequences(prim)), 100)

dir.create(file.path(dpath, "noprimers", "filtered"))
filt <- sort(file.path(dpath, "noprimers", "filtered", basename(pacF)))
for (i in 1:length(filt)){
track <- fastqFilter(fn = nop[i], fout = filt[i], minQ=3, minLen=1000, maxLen=1600, maxN=0, rm.phix=FALSE, maxEE=2, verbose=TRUE)
}
drp <- derepFastq(filt, verbose=TRUE)

err <- learnErrors(drp, BAND_SIZE=32, multithread=TRUE, errorEstimationFunction=dada2:::PacBioErrfun) # 10s of seconds
dd <- dada(drp, err=err, BAND_SIZE=32, multithread=TRUE) # seconds

seqtab <- makeSequenceTable(dd)
summary((nchar(getSequences(seqtab))))
hist(nchar(colnames(seqtab)))

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
sum(seqtab.nochim)/sum(seqtab)
dim(seqtab.nochim)


taxa <- assignTaxonomy(seqtab.nochim, "~/Downloads/silva_nr_v132_train_set.fa.gz", multithread=TRUE,)
taxa_silva <- assignSpecies(taxa, "~/Downloads/silva_species_assignment_v132.fa.gz", allowMultiple = T)
taxa_silva_both <- assignSpecies(taxa, "./docs/microbiome_data/combined.fasta", allowMultiple = T)


together <- merge(by="seq", all=TRUE,
                  data.frame(focus_genus=as.character(taxa_silva_both[,1]),
                             focus_species=as.character(taxa_silva_both[,2]),
                             seq = row.names(taxa_silva_both),
                             stringsAsFactors=FALSE),
                  data.frame(genus=as.character(taxa_silva[,1]),
                             species=as.character(taxa_silva[,2]),
                             seq = row.names(taxa_silva),
                             stringsAsFactors=FALSE)
)
                  
taxa_silva_both_noname <- taxa_silva_both
rownames(taxa_silva_both_noname) <- NULL
taxa_silva_noname <- taxa_silva
rownames(taxa_silva_noname) <- NULL

taxa_silva_noname[is.na(taxa_silva_noname)] <- ""
taxa_silva_both_noname[is.na(taxa_silva_both_noname)] <- ""

table(taxa_silva_both_noname[,1] == taxa_silva_noname[, 1], useNA="always")
table(taxa_silva_both_noname[,2] == taxa_silva_noname[, 2], useNA="always")



```