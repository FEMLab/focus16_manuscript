---
title: "Assessing Taxa Assignment with the silva plus dataset"
author: "Ben Nolan and  Nicholas Waters"
date: "last update: `r format(Sys.Date(), format = '%d %B %Y')`" 
output: html_document
editor_options: 
  chunk_output_type: console
---

# Overview
To assess the effect of using the auggmented  dataset, we show  the  taxanomic assignment for 4 dataset:  DADA2's "Balanced", "HMP", and "Extremes", and data from the recent "Endobiota" study.

The preprocessing of all the data is preformed in the next four sections, and then the final section describes combining all the sequneces into one sequence table matrix for simplicity of manipulation of the results, while keeping track of which sequence comes from where.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dada2); packageVersion("dada2")
library(ShortRead); packageVersion("ShortRead")
library(Biostrings); packageVersion("Biostrings")
SPEED = TRUE  # use  pregenerated files, dont plot if you can avoid it
dirpath <- "./docs/microbiome_data/"

```
Here, we download the data from the Endobiota study.

```{bash}
while read x; do fasterq-dump --split-files $x ; done < ~/Downloads/SRR_Acc_List-1.txt

```
# Endobiota study
We follow a very generic DADA2 workflow:
```{r}

filtpath <- "./docs/microbiome_data/clean/"
mb_meta <- read.csv2("./docs/microbiome_data/metadata.tsv", stringsAsFactors = F,sep="\t",comment.char = '#')
sra_meta <- read.csv("./docs/microbiome_data/SraRunTable-PRJEB26800.txt", stringsAsFactors = F,sep=",")
meta <- full_join(
  mb_meta %>%select(sample_alias, sample_description),
  sra_meta %>% select(Run, Sample.Name), by=c("sample_alias"="Sample.Name")
)
meta$site <- gsub("(.*?) .*", "\\1", meta$sample_description)
meta$status <- gsub(".*\\((.*?)\\)$", "\\1", meta$sample_description)

# Forward and reverse fastq filenames have format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq
endo_fnFs <- sort(list.files(dirpath, pattern="_1.fastq.gz", full.names = TRUE))
endo_fnRs <- sort(list.files(dirpath, pattern="_2.fastq.gz", full.names = TRUE))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names.endo <- sapply(strsplit(basename(endo_fnRs), "_"), `[`, 1)
```

```{r eval=!SPEED}
ggsave(plotQualityProfile(endo_fnFs), filename = file.path(dirpath, "end_F_q.png"),dpi = 300, width = 16, height = 10, units = "in")
ggsave(plotQualityProfile(endo_fnRs), filename = file.path(dirpath, "end_R_q.png"),dpi = 300, width = 16, height = 10, units = "in")

# We noticed that one of the datasets should be removed, as the seqeuncing quality appears severly impaired
baddiesF <-c(sample.names.endo[grep("009",sample.names.endo)])
baddiesR <-c(sample.names.endo[grep("009",sample.names.endo)])

sample.names.endo <- sample.names.endo[!sample.names.endo %in% unique(c(baddiesF, baddiesR))]
for (baddie in c(baddiesF, baddiesR)){
  endo_fnFs <- endo_fnFs[!grepl(baddie, endo_fnFs)]
  endo_fnRs <- endo_fnRs[!grepl(baddie, endo_fnRs)]
}
# Place filtered files in filtered/ subdirectory
endo_filtFs <- file.path(dirpath, "filtered", paste0(sample.names.endo, "_F_filt.fastq.gz"))
endo_filtRs <- file.path(dirpath, "filtered", paste0(sample.names.endo, "_R_filt.fastq.gz"))
names(endo_filtFs) <- sample.names.endo
names(endo_filtRs) <- sample.names.endo


endo_out <- filterAndTrim(
  endo_fnFs, endo_filtFs, 
  endo_fnRs, endo_filtRs,
  trimLeft=30, trimRight=40,
  maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
  compress=TRUE, multithread=TRUE)


endo_errF <- learnErrors(endo_filtFs, multithread=TRUE)
endo_errR <- learnErrors(endo_filtRs, multithread=TRUE)
plotErrors(endo_errF, nominalQ=TRUE)
endo_derepF <- derepFastq(endo_filtFs, verbose=TRUE)
endo_derepR <- derepFastq(endo_filtRs, verbose=TRUE)

endo_dadaFs <- dada(endo_derepF, err=endo_errF, multithread=TRUE)
endo_dadaRs <- dada(endo_derepR, err=endo_errR, multithread=TRUE)

endo_merger <- mergePairs(endo_dadaFs, endo_derepF, endo_dadaRs, endo_derepR, verbose=TRUE)

endo_seqtab <- makeSequenceTable(endo_merger)
summary((nchar(getSequences(endo_seqtab))))
hist(nchar(colnames(endo_seqtab)))

# here we trim some of those shorter seqeunces
endo_seqtab2 <- endo_seqtab[,nchar(colnames(endo_seqtab)) %in% 360:450]
hist(nchar(colnames(endo_seqtab2)))
dim(endo_seqtab2)
# [1]    83 23152
endo_seqtab.nochim <- removeBimeraDenovo(endo_seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
sum(endo_seqtab.nochim)/sum(endo_seqtab2)
# [1] 0.9577382
dim(endo_seqtab.nochim)
#  [1]   83 3876

getN <- function(x) sum(getUniques(x))
endo_track <- cbind(endo_out, sapply(endo_dadaFs, getN), sapply(endo_dadaRs, getN), sapply(endo_merger, getN), rowSums(endo_seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(endo_track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(endo_track) <- sample.names.endo
head(endo_track)

#save.image("./endo.RData")
save(endo_seqtab.nochim, file = "./endo_seqtab.RData")
```



# Extremes dataset ##################################
Here, we larely use the parameters defined in the original paper, but we updated to use the commands recommended in the tutorial for version  1.12.
```{r exec=!SPEED}
extremes_path <-  "."
ex_fnF <- file.path(extremes_path, "SRR2990088_1.fastq")
ex_fnR <- file.path(extremes_path, "SRR2990088_2.fastq")

ggsave(plotQualityProfile(ex_fnF), filename = file.path(dirpath, "ex_F_q.png"),dpi = 300, width = 16, height = 10, units = "in")
ggsave(plotQualityProfile(ex_fnR), filename = file.path(dirpath, "ex_R_q.png"),dpi = 300, width = 16, height = 10, units = "in")
#  Forward reads are reasonably high quality. Trimming the first 20 nts, and last 10 (truncate at 240).
# Reverse read quality drops off substantially. Trimming the first 20 nts, and last 50 (truncate at 200).

ex_filtF <- "ExtremeF_EE2.fastq.gz"
#ex_filtFO <- "ExtremeFO_EE2.fastq.gz"
ex_filtR <- "ExtremeR_EE2.fastq.gz"
ex_out <- filterAndTrim(
  fwd = ex_fnF, filt = ex_filtF, 
  rev = ex_fnR, filt.rev = ex_filtR, 
  maxN=0, maxEE=2, truncQ=2, 
  truncLen=c(240,200), trimLeft=c(20,20), compress=TRUE, verbose=TRUE)
# Kept about 60 percent of the paired reads and 70 percent of the forward-only reads.

## Run DADA2 Pipeline
ex_errF <- learnErrors(ex_filtF, multithread=TRUE)
ex_errR <- learnErrors(ex_filtR, multithread=TRUE)
ex_dadaF <- dada(ex_filtF, err=ex_errF, multithread=TRUE)
ex_dadaR <- dada(ex_filtR, err=ex_errR, multithread=TRUE)
ex_mergers <- mergePairs(ex_dadaF, ex_filtF, ex_dadaR, ex_filtR, verbose=TRUE)

ex_seqtab <- makeSequenceTable(ex_mergers)

hist(nchar(colnames(ex_seqtab)))
ex_seqtab.nochim <- removeBimeraDenovo(ex_seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
sum(ex_seqtab.nochim)/sum(ex_seqtab)
dim(ex_seqtab.nochim)
save(ex_seqtab.nochim, file = "./ex_seqtab.RData")

```


# Balanced dataset ##################################
```{r eval=!SPEED}
balanced_dir <- "~/Downloads/" # CHANGE ME to location of file
balanced_fnF <- file.path(balanced_dir, "ERR777695_1.fastq.gz")
balanced_fnR <- file.path(balanced_dir, "ERR777695_2.fastq.gz")

ggsave(plotQualityProfile(balanced_fnF), filename = file.path(dirpath, "balanced_F_q.png"),dpi = 300, width = 16, height = 10, units = "in")
ggsave(plotQualityProfile(balanced_fnR), filename = file.path(dirpath, "balanced_R_q.png"),dpi = 300, width = 16, height = 10, units = "in")

balanced_filtF <- "balanced_F.fastq.gz"
#ex_filtFO <- "ExtremeFO_EE2.fastq.gz"
balanced_filtR <- "balanced_R.fastq.gz"

balanced_out <- filterAndTrim(
  fwd = balanced_fnF, filt = balanced_filtF, 
  rev = balanced_fnR, filt.rev = balanced_filtR, 
  maxN=0, maxEE=2, truncQ=2, 
  trimLeft=c(10,10), trimRight = c(40,40), 
  compress=TRUE, verbose=TRUE)

balanced_errF <- learnErrors(balanced_filtF, multithread=TRUE)
balanced_errR <- learnErrors(balanced_filtR, multithread=TRUE)
balanced_dadaF <- dada(balanced_filtF, err=balanced_errF, multithread=TRUE)
balanced_dadaR <- dada(balanced_filtR, err=balanced_errR, multithread=TRUE)
balanced_mergers <- mergePairs(balanced_dadaF, balanced_filtF, balanced_dadaR, balanced_filtR, verbose=TRUE)

balanced_seqtab <- makeSequenceTable(balanced_mergers)

hist(nchar(colnames(balanced_seqtab)))
balanced_seqtab.nochim <- removeBimeraDenovo(balanced_seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
sum(balanced_seqtab.nochim)/sum(balanced_seqtab)
dim(balanced_seqtab.nochim )
save(balanced_seqtab.nochim, file = "./balanced_seqtab.RData")

```

# HMP
```{r eval=!SPEED}
hmp_dir <- "~/Downloads/130403" # CHANGE ME to location of file
hmp_fnF <- file.path(hmp_dir, "Mock1_S1_L001_R1_001.fastq.bz2")
hmp_fnR <- file.path(hmp_dir, "Mock1_S1_L001_R2_001.fastq.bz2")

ggsave(plotQualityProfile(hmp_fnF), filename = file.path(dirpath, "hmp_F_q.png"),dpi = 300, width = 16, height = 10, units = "in")
ggsave(plotQualityProfile(hmp_fnR), filename = file.path(dirpath, "hmp_R_q.png"),dpi = 300, width = 16, height = 10, units = "in")
# the reverse reeds dont look great
hmp_filtF <- "hmp_F.fastq.gz"
#ex_filtFO <- "ExtremeFO_EE2.fastq.gz"
hmp_filtR <- "hmp_R.fastq.gz"

hmp_out <- filterAndTrim(
  fwd = hmp_fnF, filt = hmp_filtF, 
  rev = hmp_fnR, filt.rev = hmp_filtR, 
  maxN=0, maxEE=2, truncQ=2, 
  truncLen=c(240,200), trimLeft=c(20,20),
  compress=TRUE, verbose=TRUE)

hmp_errF <- learnErrors(hmp_filtF, multithread=TRUE)
hmp_errR <- learnErrors(hmp_filtR, multithread=TRUE)
hmp_dadaF <- dada(hmp_filtF, err=hmp_errF, multithread=TRUE)
hmp_dadaR <- dada(hmp_filtR, err=hmp_errR, multithread=TRUE)
hmp_mergers <- mergePairs(hmp_dadaF, hmp_filtF, hmp_dadaR, hmp_filtR, verbose=TRUE)

hmp_seqtab <- makeSequenceTable(hmp_mergers)

hist(nchar(colnames(hmp_seqtab)))
hmp_seqtab.nochim <- removeBimeraDenovo(hmp_seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
sum(hmp_seqtab.nochim)/sum(hmp_seqtab)
dim(hmp_seqtab.nochim )
save(hmp_seqtab.nochim, file = "./hmp_seqtab.RData")

```


# Combine sequence tables

```{r}
if (SPEED){
  for (p  in c("hmp_seqtab.RData", "ex_seqtab.RData", "balanced_seqtab.RData", "endo_seqtab.RData")){
    load(p)
  }
}

# Now, we format them so we can bind them all together
#View(hmp_seqtab.nochim)
#View(endo_seqtab.nochim)

rownames(hmp_seqtab.nochim) <- "HMP"
rownames(ex_seqtab.nochim) <- "Extremes"
rownames(balanced_seqtab.nochim) <- "Balanced"

thesenames <- c(
  rownames(endo_seqtab.nochim), 
  rownames(hmp_seqtab.nochim),
  rownames(ex_seqtab.nochim),
  rownames(balanced_seqtab.nochim)
)
combined_seqtab <- dplyr::bind_rows(
  as.data.frame(endo_seqtab.nochim, row.names = rownames(endo_seqtab.nochim)),
  as.data.frame(hmp_seqtab.nochim,  row.names = rownames(hmp_seqtab.nochim)),
  as.data.frame(ex_seqtab.nochim,  row.names = rownames(ex_seqtab.nochim)),
  as.data.frame(balanced_seqtab.nochim,  row.names = rownames(balanced_seqtab.nochim)),
    
)
rownames(combined_seqtab) <- thesenames
combined_seqtab <- as.matrix(combined_seqtab)
#combined_seqtab <-  combined_seqtab[, 1:40]
```

Now, we need to reformat the SILVA database for use with dada2. We could use the prebuild one, but as we have to build 2 (the normal SILVA 132 and our augmented one), we outline the steps here.

we download the SILVA tax file:
https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/SILVA_132_SSURef_tax_silva.fasta.gz

And the alignment file.
https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/SILVA_132_SSURef_Nr99_tax_silva_full_align_trunc.fasta.gz
 
We recreate the databases described here: https://zenodo.org/record/1172783, but to do that, we need to add our seqeunces to the alignment, which we can do with MAFFT. 



```{bash eval=FALSE}

mkdir ~/Downloads/dada2_dbs/

zcat < ~/Downloads/SILVA_132_SSURef_tax_silva.fasta.gz | sed 's/>/> /g' | cut -f 1,3 -d " " | sed 's/ //g'  | awk '/^>/ {$0=$0 ";"}1' |gzip -c > ~/Downloads/dada2_dbs/silva.132.formated.fasta.gz
zcat < ~/Downloads/dada2_dbs/silva.132.formated.fasta.gz | awk 'BEGIN{RS=">";FS="\n"}NR>1{printf ">%s\n",$1;for (i=2;i<=NF;i++) {gsub(/U/,"T",$i); printf "%s\n",$i}}' |gzip -c > ~/Downloads/dada2_dbs/silva.132.dna.formated.fasta.gz

cp  ~/Downloads/SILVA_132_SSURef_tax_silva.fasta.gz  ~/Downloads/SILVA_132_SSURef_tax_silva_plus.fasta.gz
cat ./results/fast_focusDB_ribo16s.fasta |gzip -c  >>  ~/Downloads/SILVA_132_SSURef_tax_silva_plus.fasta.gz
zcat < ~/Downloads/SILVA_132_SSURef_tax_silva_plus.fasta.gz | sed 's/>/> /g' | cut -f 1,3 -d " " | sed 's/ //g'  | awk '/^>/ {$0=$0 ";"}1'  | gzip -c > ~/Downloads/dada2_dbs/silva.132.plus.formated.fasta.gz
zcat < ~/Downloads/dada2_dbs/silva.132.plus.formated.fasta.gz | awk 'BEGIN{RS=">";FS="\n"}NR>1{printf ">%s\n",$1;for (i=2;i<=NF;i++) {gsub(/U/,"T",$i); printf "%s\n",$i}}' |gzip -c > ~/Downloads/dada2_dbs/silva.132.plus.formated.dna.fasta.gz


```


Now, we can use DADA2's built-in command to create a species level dbs for both:
```{r, eval=FALSE}
dada2:::makeSpeciesFasta_Silva("~/Downloads/SILVA_132_SSURef_tax_silva.fasta.gz", "~/Downloads/dada2_dbs/silva_species_assignment_v132.fa.gz")
dada2:::makeSpeciesFasta_Silva("~/Downloads/SILVA_132_SSURef_tax_silva_plus.fasta.gz", "~/Downloads/dada2_dbs/silva_plus_species_assignment_v132.fa.gz")
#313502 sequences with genus/species binomial annotation output.
#319084 sequences with genus/species binomial annotation output.
```


That aside,we can assign taxanomic levels to each
```{r, cache=TRUE, eval=FALSE}
taxa_silva  <- assignTaxonomy(combined_seqtab, "~/Downloads/dada2_dbs/silva.132.dna.formated.fasta.gz", multithread=TRUE, verbose = T)
taxa_silva_species <- assignSpecies(taxa_silva,verbose = T, "~/Downloads/dada2_dbs/silva_species_assignment_v132.fa.gz", allowMultiple = T)
# 710 out of 4098 were assigned to the species level.

taxa_silva_plus  <- assignTaxonomy(combined_seqtab, "~/Downloads/dada2_dbs/silva.132.plus.formated.dna.fasta.gz", multithread=TRUE, verbose = T)
taxa_silva_plus_species <- assignSpecies(taxa_silva_plus, "~/Downloads/dada2_dbs/silva_plus_species_assignment_v132.fa.gz", allowMultiple = T, verbose = T)
# 713 out of 4098 were assigned to the species level.

save.image("taxa_assigned.RData")
```

Now we have taxa matrices for the data with both the SILVA db and our SILVA+ db created with focusDB.  How did it change the assignment?


```{r}
together <- merge(
  by="seq", all=TRUE, 
  data.frame(silva=paste(
    as.character(taxa_silva_species[,1]),
    as.character(taxa_silva_species[,2])),
    seq = row.names(taxa_silva_species),
    stringsAsFactors=FALSE),
  data.frame(focuDB=paste(
    as.character(taxa_silva_plus_species[,1]),
    species=as.character(taxa_silva_plus_species[,2])),
    seq = row.names(taxa_silva_plus_species),
    stringsAsFactors=FALSE)
)
together[together$silva!=together$focuDB, ] %>% View()


```







The first time we ran the analysis (before having the focusDB results), here is what we did to generate the species files, etc:
```{r, eval=FALSE}
taxa.print <- endo_taxa_silva # Removing sequence rownames for display only
taxa.print[is.na(taxa.print[,1])] <- ""
taxa.print[is.na(taxa.print[,2])] <- ""
thesenames <- gsub("^ $", "", paste(taxa.print[,1], taxa.print[,2] ))
table(thesenames)

write.table(sort(unique(thesenames)), file = "./docs/microbiome_data/endo_species.txt", row.names = F,  col.names = F, quote=F)
fg <-  data.frame(taxa[,c(5,6)], stringsAsFactors = F)
rownames(fg) <- NULL
fg$cleangenus <- gsub("[-_](.*)", "", fg$Genus)
fg$cleanfamily <- gsub("[-_](.*)", "", fg$Family)
fg$cleangenus <- ifelse(grepl("\\d", fg$cleangenus),fg$cleanfamily, fg$cleangenus)
fg$cleangenus <- ifelse(is.na(fg$cleangenus),fg$cleanfamily, fg$cleangenus)
write.table(sort(unique(fg$cleangenus)), file = "./docs/microbiome_data/endo_genus.txt", row.names = F,  col.names = F, quote=F)
```


## Running FocusDB

Now that we know what genomes to be expecting, we ran focusDB on the genera found, and combined the results into a new database.


## Combining the DB:

```{r}
gg <- read.table("./docs/microbiome_data/endo_species.txt")
tt <-taxa_silva[1:10, ]


together <- merge(
  by="seq", all=TRUE, 
  data.frame(silva=paste(
    as.character(endo_taxa_silva_species[,1]),
    as.character(endo_taxa_silva_species[,2])),
    seq = row.names(endo_taxa_silva_species),
    stringsAsFactors=FALSE),
  data.frame(focuDB=paste(
    as.character(endo_taxa_silva_plus_species[,1]),
    species=as.character(endo_taxa_silva_plus_species[,2])),
    seq = row.names(endo_taxa_silva_plus_species),
    stringsAsFactors=FALSE)
)
together[together$silva!=together$focuDB, ]
```

# save for rerunning:

```{r}
#save.image("./microbiome.RData")
```


## Long length read data

With short read data, we do not identify more sequences, but we wantred to see how it would perfom on full-length 16s from the same physilogical location. We identified a  dattaset from wagner 2016: <https://bmcmicrobiol.biomedcentral.com/articles/10.1186/s12866-016-0891-4>.  We downloaded those reads from ENA, and converted to fastq with PacBio's <https://github.com/PacificBiosciences/pbh5tools>.

```{bash, eval=FALSE} 
conda create -n pacdada pbh5tools bax2bam lima ; conda activate pacdada

bax2bam -o microbiome ./m150128_105130_00127_c100679732550000001823135702221544_s1_p0.bas.h5


bash5tools.py  --outFilePref myreads --outType fastq  --minLength 1400 ./m150128_105130_00127_c100679732550000001823135702221544_s1_p0.bas.h5
conda deactivate 
conda create -n demul && conda activate demul && pip install demultiplex biopython==1.72

```

These were then taken through the dada2 pipeline, following practices laid out here: <https://benjjneb.github.io/LRASManuscript/LRASms_Zymo.html>, using primer seqeunces from the supplementary information:

```{r}
primers <-  read.csv2(text="PRIMER_NAME BARCODE PRIMER
BACT16s_v1_0001_Forward TCAGACGATGCGTCAT AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0002_Forward CTATACATGACTCTGC AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0003_Forward TACTAGAGTAGCACTC AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0004_Forward TGTGTATCAGTACATG AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0005_Forward ACACGCATGACACACT AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0006_Forward GATCTCTACTATATGC AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0007_Forward ACAGTCTATACTGCTG AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0008_Forward ATGATGTGCTACATCT AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0009_Forward CTGCGTGCTCTACGAC AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0010_Forward GCGCGATACGATGACT AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0011_Forward CGCGCTCAGCTGATCG AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0012_Forward GCGCACGCACTACAGA AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0013_Forward ACACTGACGTCGCGAC AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0014_Forward CGTCTATATACGTATA AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0015_Forward ATAGAGACTCAGAGCT AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0016_Forward TAGATGCGAGAGTAGA AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0017_Forward CATAGCGACTATCGTG AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0018_Forward CATCACTACGCTAGAT AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0019_Forward CGCATCTGTGCATGCA AGMGTTYGATYMTGGCTCAG
BACT16s_v1_0020_Forward TATGTGATCGTCTCTC AGMGTTYGATYMTGGCTCAG
BACT16s_v9_0001_Reverse ATGACGCATCGTCTGA ACGGYTACCTTGTTACGACTT
BACT16s_v9_0002_Reverse GCAGAGTCATGTATAG ACGGYTACCTTGTTACGACTT
BACT16s_v9_0003_Reverse GAGTGCTACTCTAGTA ACGGYTACCTTGTTACGACTT
BACT16s_v9_0004_Reverse CATGTACTGATACACA ACGGYTACCTTGTTACGACTT
BACT16s_v9_0005_Reverse AGTGTGTCATGCGTGT ACGGYTACCTTGTTACGACTT
BACT16s_v9_0006_Reverse GCATATAGTAGAGATC ACGGYTACCTTGTTACGACTT
BACT16s_v9_0007_Reverse CAGCAGTATAGACTGT ACGGYTACCTTGTTACGACTT
BACT16s_v9_0008_Reverse AGATGTAGCACATCAT ACGGYTACCTTGTTACGACTT
BACT16s_v9_0009_Reverse GTCGTAGAGCACGCAG ACGGYTACCTTGTTACGACTT
BACT16s_v9_0010_Reverse AGTCATCGTATCGCGC ACGGYTACCTTGTTACGACTT
BACT16s_v9_0011_Reverse CGATCAGCTGAGCGCG ACGGYTACCTTGTTACGACTT
BACT16s_v9_0012_Reverse TCTGTAGTGCGTGCGC ACGGYTACCTTGTTACGACTT
BACT16s_v9_0013_Reverse GTCGCGACGTCAGTGT ACGGYTACCTTGTTACGACTT
BACT16s_v9_0014_Reverse TATACGTATATAGACG ACGGYTACCTTGTTACGACTT
BACT16s_v9_0015_Reverse AGCTCTGAGTCTCTAT ACGGYTACCTTGTTACGACTT
BACT16s_v9_0016_Reverse TCTACTCTCGCATCTA ACGGYTACCTTGTTACGACTT
BACT16s_v9_0017_Reverse CACGATAGTCGCTATG ACGGYTACCTTGTTACGACTT
BACT16s_v9_0018_Reverse ATCTAGCGTAGTGATG ACGGYTACCTTGTTACGACTT
BACT16s_v9_0019_Reverse TGCATGCACAGATGCG ACGGYTACCTTGTTACGACTT
BACT16s_v9_0020_Reverse GAGAGACGATCACATA ACGGYTACCTTGTTACGACTT
", sep=" ", stringsAsFactors=F)
write.table(primers, file = "microbiome_primers.tab", sep = "\t", quotes=F, row.names = F)
```

As the forward barcode is the same as the reverese, we reformed the primers into a fasta file required by lima <https://github.com/PacificBiosciences/barcoding>.  We then brought the strains through a bax2bam  -> lima  -> bam2fastq pipeline

```{bash eval=FALSE}
conda install  pbccs==3.4 bax2bam bam2fastx lima
for i in ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR144/ERR1447466/m150128_105130_00127_c100679732550000001823135702221544_s1_p0.2.bax.h5 ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR144/ERR1447466/m150128_105130_00127_c100679732550000001823135702221544_s1_p0.3.bax.h5 ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR144/ERR1447466/m150128_105130_00127_c100679732550000001823135702221544_s1_p0.bas.h5 ftp://ftp.sra.ebi.ac.uk/vol1/run/ERR144/ERR1447466/m150128_105130_00127_c100679732550000001823135702221544_s1_p0.metadata.xml ; do wget $i ; done

bax2bam -o microbiome ./m150128_105130_00127_c100679732550000001823135702221544_s1_p0.bas.h5
ccs ./microbiome.subreads.bam microbiome.ccs.bam
lima --log-level TRACE --same -ccs  ./microbiome.ccs.bam ./primers.fasta ./demuxed_same_ccs.bam
bam2fastq --split-barcodes -o ERS1190962_ccs  ./demuxed_same_ccs.bam
mkdir ERS1190962_ccs/
mv ERS1190962_ccs*.fastq.gz ./ERS1190962_ccs/
tar czf ERS1190962_css.tar.gz ERS1190962_ccs/

# download to local computer from $SCRATCH/pacbio

```


Then, we follow the tutorial:

```{r}
dpath <- "~/Downloads/ERS1190962_ccs_limits/" # CHANGE ME to location of the fastq file
Fprimer <- "AGMGTTYGATYMTGGCTCAG"
Rprimer <- "ACGGYTACCTTGTTACGACTT"

#rc <- dada2:::rc
# theme_set(theme_bw())
# genusPalette <- c(Bacillus="#e41a1c", Enterococcus="#377eb8", Escherichia="#4daf4a", Lactobacillus="#984ea3",
#                   Listeria="#ff7f00", Pseudomonas="#ffff33", Salmonella="#a65628", Staphylococcus="#f781bf")
pacF <- sort(list.files(dpath, pattern=".fastq.gz", full.names = TRUE))
pac.sample.names <- sapply(strsplit(basename(pacF), "_"), `[`, 3)

nop <- sort(file.path(dpath, "noprimers", basename(pacF)))
prim <- removePrimers(pacF, nop, primer.fwd=Fprimer, primer.rev=dada2:::rc(Rprimer), orient=TRUE, verbose=TRUE)
#hist(nchar(dada2::getSequences(prim)), 100)

dir.create(file.path(dpath, "noprimers", "filtered"))
filt <- sort(file.path(dpath, "noprimers", "filtered", basename(pacF)))
for (i in 1:length(filt)){
track <- fastqFilter(fn = nop[i], fout = filt[i], minQ=3, minLen=1400, maxLen=1600, maxN=0, rm.phix=FALSE, maxEE=2, verbose=TRUE)
}
drp <- derepFastq(filt, verbose=TRUE)

err <- learnErrors(drp, BAND_SIZE=32, multithread=TRUE, errorEstimationFunction=dada2:::PacBioErrfun) # 10s of seconds
dd <- dada(drp, err=err, BAND_SIZE=32, multithread=TRUE) # seconds

seqtab <- makeSequenceTable(dd)
summary((nchar(getSequences(seqtab))))
hist(nchar(colnames(seqtab)))

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
sum(seqtab.nochim)/sum(seqtab)
dim(seqtab.nochim)


taxa <- assignTaxonomy(seqtab.nochim, "~/Downloads/silva_nr_v132_train_set.fa.gz", multithread=TRUE,)
taxa_silva <- assignSpecies(taxa, "~/Downloads/silva_species_assignment_v132.fa.gz", allowMultiple = T)
taxa_silva_both <- assignSpecies(taxa, "./docs/microbiome_data/combined.fasta", allowMultiple = T)


together <- merge(by="seq", all=TRUE,
                  data.frame(focus_genus=as.character(taxa_silva_both[,1]),
                             focus_species=as.character(taxa_silva_both[,2]),
                             seq = row.names(taxa_silva_both),
                             stringsAsFactors=FALSE),
                  data.frame(genus=as.character(taxa_silva[,1]),
                             species=as.character(taxa_silva[,2]),
                             seq = row.names(taxa_silva),
                             stringsAsFactors=FALSE)
)
                  
taxa_silva_both_noname <- taxa_silva_both
rownames(taxa_silva_both_noname) <- NULL
taxa_silva_noname <- taxa_silva
rownames(taxa_silva_noname) <- NULL

taxa_silva_noname[is.na(taxa_silva_noname)] <- ""
taxa_silva_both_noname[is.na(taxa_silva_both_noname)] <- ""

table(taxa_silva_both_noname[,1] == taxa_silva_noname[, 1], useNA="always")
table(taxa_silva_both_noname[,2] == taxa_silva_noname[, 2], useNA="always")



```